\section{Plans and intentions in philosophy}
The humanity has long been obsessed with the idea of animating the inanimate. Perhaps not surprisingly, guiding action has proven harder than mere animation. Intentional action. Intention guides action. But how to characterize intention? It is crucial to agency.

\subsection{Agency, autonomy and action}
Agents have a capacity to act. The standard conception of agency is intentional action. Philosophy of action strives to say something about the agency. Action is  intentionality

Bratman on temporal agency.
\subsubsection{Free will}
\subsubsection{People are future oriented}
\subsection{Intentions and intentionality}
\subsubsection{Intentions are practical attitudes}
Intentions are practical attitudes as opposed to theoretical and affective attitudes.


Searle: distinction between prospective intention and intention in action (1983, p. 84-85).

Intentions as plans has been Michael Bratman

Intention can be taken to mean at least three things: intention for the future, intention with which and intentional action. A fleshed out account of intention should accommodate for these meanings and their relations.

One of the backbones of Bratman's planning theory is the claim that future-directed intentions are primary to intention-in-action. Bratman argues that first and foremost, intentions are plan states connected to our temporally extended agency. Plans are hierarchically structured so that often plans concerning means are nested in plans concerning ends. These structures are also partial in that they often aren't mapped all the way down; our plans don't have to be detailed all the way to specific acts, even though we might recognize that eventually it will be necessary. \citep*[p.~15]{bratman_shared_2014}

For Bratman, norms of intention rationality descriptively showcase the commonalities of human intentional action and their normative force plays a part in explaining the stability of intentions. \citep*[p.~16]{bratman_shared_2014}

\subsubsection{Philosophy of Mind: what are people like?}
This is an empirical question, answered partly by psychology and cognitive science.
\subsubsection{Philosophy of Action: ??}
\subsubsection{Ethics: which acts are intentional?}
\subsubsection{Do intentions constrain choice? (Bratman vs Sinhababu)}
Michael Bratman’s theory of human practical reasoning \citep{bratman_intention_1987, bratman_shared_2014}, Belief-Desire-Intention or BDI for short, has already greatly influenced model building in computer science in the field of artificial intelligence, in which it is often implemented as the belief–desire–intention software model. Bratman's model is an extension of the Belief-Desire, or BD model, which is widely used especially in game theory. Intentions are desires that the agent has already chosen to do, based on his beliefs.

\subsubsection{Is intention needed at all?}
Sinhababu argues that the belief-desire model can account for intention, thus Bratman's intention isn't needed at all.

But how does a desire give birth to a belief that we will eventually do what we want? Sinhababu doesn’t explicate the mechanism. Granted, any psychological mechanism could be easily added into the account. There could be some circularity though: “All that matters for exclusion is that when we intend to do something in the future, we form the belief that we’ll do it.”
As a causal claim this would be circular: an intention (an intentional desire perhaps) leads to a corresponding belief and at the same time a desire qualifies as an intention only with the corresponding belief. As a requirement.
Exclusion doesn’t have to be a fact, it can be a norm. This allows for failure.

Even if Sinhababu is right, there seems to be something alluring about the concept of intention. Do beliefs about our own plans differ from other kinds of beliefs? Their truth-values depend on facts, but crucially their truth also hangs on our own resolve. It’s very much in our own hands. In some sense they are very fidgety: in one second they can be true, and false in another.
Is a bad plan still a plan?

Bratman:
As Bratman (1999, p. 15) notes, the desire-belief model articulates a normative conception of practical rationality at the same time as it attempts to describe the piece of the human mind relevant to intuition and action.

Volitional commitment
Conduct-controlling pro-attitudes vs potential influencers of action
“settling”
According to Sinhababu if desire-belief pairs are sufficient for motivation they are conduct-controlling. However, Bratman does make a distinction between conduct-controlling pro-attitudes and pro-attitudes as potential influencers of action. Motivation for Sinhababu seems to be a potential influencer rather than conduct-controlling (Sinhababu 2013, footnote 25). This alone makes the point somewhat moot.
Furthermore, related to the previous issue, Bratman’s point (which Sinhababu fails to see “If there’s some problem with desire-belief pairs controlling conduct, Bratman hasn’t told us what it is” even when he quotes Bratman on settling) is that having a desire and a relevant means-ends belief doesn’t settle the issue. According to Sinhababu’s account, we act on our strongest motivation, the highest combined (calculated) means-ends probability and the strength of our desire. Bratman acknowledges that having a desire and a corresponding belief provides a reason to act, but also claims that it doesn’t settle the issue. Acting on our strongest motivation would surely be rational, but people are not always rational (though I think Sinhababu says so in his paper as well). Sinhababu’s account thus seems more normative than descriptive.
“In requiring that I desire to go more than I desire to perform actions thought incompatible with going, the account makes it impossible to intend to perform actions thought incompatible. But while it is, other things equal, criticizably irrational to have such conflicting intentions, it does seem possible to be guilty of such irrationality. And we want our theory to allow for this possibility.” Bratman p. 19
Two kinds of beliefs: 1) means-ends belief about the possibility of the intended issue and 2) a belief about my future actions.

\subsection{Collective intentionality}
BRATMAN

Bratman's central contribution is his theory of collective intentionality. Another difference between theories is the location of collectivity in collective intentions, where in the intention does the collectivity reside. Content, mode and subject.

For Bratman collectivity is in the content of intentions: "I intend that we J." For Bratman collectivity also requires common knowledge and more than one people intending that we J with meshing subplans. The benefits of situating collectivity into the content of intentions are...

Intended plans, into which the agent is committed, are 1) driving factors of means-end reasoning, 2) constraints on options, and also 3) influence beliefs about the future \citep*[p.~9]{bratman_plans_1988}. Or are committed plans real beliefs rather than just things that influence beliefs? \todo[inline]{on second thought, nah, why would they be beliefs?}

Of course at least in the planning process and probably later too, plans are weaker than beliefs. While a change of belief can make a plan impossible, a plan can't change in a way that would make beliefs impossible. This might be just due to the fact that plans are always about the future. Another, maybe bigger, factor is of course the fact that I am relatively free to change my plans (Or am I really? Maybe this is just due to the fact that they are in the future. Of course some empirical facts can be changed by the agent as well, thus changing the belief as well.), but if I want to be rational, I can't change my beliefs.
\todo[inline]{An intriguing subject: the difference between beliefs about past, present and future.}

To constrain, plans must be stable. To be stable, plans (especially about the far future) need to be partial (temporally and/or structurally). Structural partiality of plans allows for their decomposition into intentions. Eventually agents "fill in" their partial plans.

This analysis is super interesting since it seems to show the very real differences between people. I think that a big factor of conflict in relationships is that some people allow for way more partiality in their plans than others. One resulting problem is that to the stringent partner the requirement of consistency is often violated, whereas to the non-stringent partner the available options are unnecessarily constrained. Neither has to be overly stringent or overly non-stringent. \citep*[p.~20]{bratman_plans_1988} use similar terms, \textit{cautious} and \textit{bold}, to denote the treshold of the filter override (I guess this is not the same thing I discussed above?). 

The requirement of consistency (or means-ends coherence) in plans as a way of means-ends reasoning seems to lead inevitably to the role of plans as a constraint.

SEARLE AND TUOMELA

For Searle and Tuomela, collectivity resides in the mode of the intention. The mode of we-intentions is collective whereas the mode of I-intentions is not. We-intentions are intentions about us. A group has a joint intention if the agents in the group have similar we-intentions and mutual belief. The benefits of situating collectivity into the mode of intentions are...

GILBERT

For Margaret Gilbert, collectivity resides in the subject of intentions (or commitments): if A and B have a joint commitment, they constitute a plural subject that is the subject of the commitment.

\section{Planning in AI}

AI is the study of rational action, which means that planning is important, "devising a plan of action to achieve one's goals".

According to this definition planning seems like straightforward means-ends reasoning. Bratman however differentiates means-ends reasoning from intention. Two conceptions of a plan. The Plan Library is a subset of the agent's beliefs \citep[p.~7]{bratman_plans_1988}, a set of (contrafactual?) conditionals. A plan is an adopted plan.

Thus it seems that planning can be the formulation of means-ends beliefs or conditional contrafactuals, in other words compiling a plan library. Or it can be the process of adopting/discarding plans as intentions. These take place in different places of the architecture diagram \citep[p.~7]{bratman_plans_1988}.

An architecture for resource-bound agents \citep[p.~7]{bratman_plans_1988} has input in perception that changes beliefs.

\subsection{The interdisciplinary field of AI}
Computer science, cognitive science...
\subsection{The planning problem}
In the field of AI, or artificial intelligence, the planning problem or the problem of automating means-end reasoning, is central when it comes to rational behavior. It is a formalized, algorithmic, mathematical excercise. In multi-agent systems formulating the preconditions, the assumptions, have common ground with many philosophical questions. In what sense might groups have plans and how do groups plan? How do individuals take groups and their (social) environment in general into account in or in the making of their plans? Multi-agent planning raises many questions about how individualist the world is and how individualist we should be.

Another problem: planning at higher levels of abstraction.

Planning can occur before or during the execution of the plan.

\subsection{Autonomous agents and complexity}

\subsubsection{Artificial agents}

Future oriented? Autonomous agents as opposed to functional systems that merely compute.

According to Wooldridge \citeyearpar[p.~8]{wooldridge_intelligent_2013} (following \citealt{wooldridge_intelligent_1995}), an agent is defined as "a system that is situated in some environment and that is capable of autonomous action in this environment in order to achieve its delegated objectives". Autonomy "is used to mean that agents are able to act without the intervention of humans or other systems: they have control both over their own internal state and over their behavior." Environments can be deterministic or non-deterministic, accessible or inaccessible, episodic or non-episodic, static or dynamic and finally discrete or continuous \citep[p.~6]{wooldridge_intelligent_2013}. Capability to act in an environment presupposes sensors for input in the form of percepts about the environment, and effectors or actuators to interact with the environment. To be able to act in all but the most trivial environments, they must be robust enough to withstand failure.

An intelligent agent is defined by Wooldridge \citeyearpar[p.~8]{wooldridge_intelligent_2013} as an agent that behaves proactively, reactively and socially in order to meet its delegated objectives. I presume that an ability to behave in such a manner is enough to satisfy the requirements of an intelligent agent. The intelligent agent can act in environments that are more complex, with uncertainty or with multiple agents capable of changing the environment.

* What does it mean that it's easy to build purely reactive systems? \citep[p.~9]{wooldridge_intelligent_2013} 

* What about the ability to dissect a goal to small, executable pieces? I guess that's really what we mean by proactiveness.

The hard part in building intelligent agents is the balance between a goal-directed system and a purely reactive system. This means that we don't want the agent to continually respond to the environment forgetting its goals. To be exact, this is already in the definition of reactivity: "in order to satisfy their delegated objectives" \citep[p.~8]{wooldridge_intelligent_2013} . Perhaps it should be omitted in order to produce the conflict between goal-directedness and reactiveness.

A big part of excessive reactiveness in humans is also being reactive to one's own internal states that one does not have direct control of, such as emotions, etc.

Finally, cooperation means not just any interaction, but the ability to negotiate and cooperate.

\textbf{Autonomy}: more usually than not, we don’t want managing a system to be a game of Lemmings, where the agents are predictable, but doomed to fail if not swiftly assisted through the various obstacles in their environment.

One factor related to autonomy is the ability to function in different environments. It’s robustness, it’s ability to withstand (and function in) a variety of environments, that are might not be specified in the beginning. This is close to reactivity \citep[p.~8]{wooldridge_intelligent_2013} .

\textbf{Transparency}: not having to reverse-engineer the system on the run in order to use it efficiently, or aid it. This is often related to human capacities and human-likeness.

\subsection{Constraining choice}
The problems of unbounded single-agent models

The need for simplificatory assumptions is often present in any model-building exercise. The main factor on most cases is our resource-boundedness. In most cases even if time is not crucial for any other reason, the environment might change in a way that demands more practical reasoning, more processing and thus more time. It allows for complexity in the form of multiple agents, changing environment.

Practical reasoning under constraint might not be desirable, but it is realistic in that every system on the planet does that. This might lead one to think that a good way is to emulate the ways living systems have adapted to reason practically under constraint. This is problematic at least for the following reasons: 1) in most / the most interesting cases we don't know exactly how living systems work, 2) there might be better ways to cope with resource-boundedness and 3) the environment for many if not most artificial agents is not similar to us and 4) the way of perceiving the world of artificial agents can differ and thus lead to different ways of parsing and arranging the concepts relevant to action.

In the architecture suggested by Bratman, Israel and Pollack \citeyearpar{bratman_plans_1988} plans constrain the amount of practical reasoning the agent must do.

In my opinion, the motivation or the goal of Bratman, Israel and Pollack in their 1988 paper was to refine such simplificatory assumptions that might trump in some aspects the much researched single-agent models.

\subsubsection{Practical reasons, tractability}
Tractability is also discussed in the  philosophy of economics.

Raimo Tuomela's theory of social action is based on the notion of we-mode. He considers acting in the we-mode as action that is motivated by the supposed intentions of any social group we belong to. Acting in I-mode, on the other hand, begins from our own intentions. Raul Hakli (forthcoming) suggests that Tuomela's we-mode thinking could be used in designing multi-agent models of artificial intelligence. According to Hakli, the standard approach is to begin from the point of view of the individual and try to build social action by making the atomistic individuals interact together. Planning in the we-mode however would enable the individuals to begin from the intentions of the group and thus ease the model-building process.

\subsubsection{Intentions}

The BDI-framework based on Bratman's work in the philosophy of social action has been adopted to AI.

Here I present some notable differences between the work of Michael Bratman and other theorists and try to evaluate if these differences are relevant to the theories' application in AI, hoping to find stuff that has been overlooked. Bratman's work is individualist and reductive which might to an extent help in the creation of implementable formalizations.

\subsection{Modeling, alogrithms}



