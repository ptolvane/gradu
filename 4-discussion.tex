\section{Contingency of intentions in humans and usefulness of intentions in AI}


\subsection{Possibility of different intentionalities in non-humans}


\subsubsection{Non-constraining intentionalities (Sinhababu)?}


\subsubsection{Ethically irrelevant intentionalities (Free will, Mele)}


\subsection{Also useful because makes machines more relatable and predictable for humans}

"Reduced need for communication, bargaining and argumentation" "may lead to more human-like behaviour and therefore make it easier for humans to interact with robots – improved ability to coordinate their actions with human beings" "behaviour that -- appears familiar to human partners" (Hakli forthcoming)

\subsubsection{Team reasoning and other planning methods}
"Also partial global planning (PGP) methods..." (Hakli forthcoming)

The difference between team reasoning and "a bottom-up strategy of starting from the plans of individual agents and then [using] some sort of plan merging method to coordinate them into a global plan" (Hakli forthcoming (or similar) springs to mind the possible way agents and groups go about forming their group intentions: by recognizing group intentions and group goals, comparing them to one's own and joining groups with similar goals or goals that resonate in some other manner with one's own values, feelings, beliefs, desires, intentions.


\subsection{Intentionality helps make machines moral?}


\section{The planning problem}
In AI and in philosophy. A philosophical take on the planning problem. What is problematic in planning or what problem(s) does planning solve? Several problems, why did I choose the definition I chose?

"Who plans the planner?" It's well known that even decision making by majority can lead to opposite results depending on the decision-making process (Pettit?).


\section{Empirical/philosophical vs practical interest toward intentions (and everything)}
There is some common ground between theory of social action and multi-agent systems. There are also differing goals, etc. Both fields are interdisciplinary, drawing from a variety of disciplines. This work is also interdisciplinary, an attempt of interdisciplinary integration.

What does it mean to have pragmatist goals / interests? Might want to check Stanford Encyclopedia.

Differing goals in the philosophy of social action and AI-related research. It goes almost without saying that the goals of AI research are more practical or pragmatic while the goals of philosophy are usually characterized as epistemic. What does this mean? On a finer mesh, one can strive towards e.g. formalization or implementation. These are not exclusive, on the contrary they can support each other: formalization can and usually will aid in implementation. Pragmaticity and epistemicity are not exclusive either, but they clearly seem to be different goals. Either the goal is pragmatic, epistemic or both, but striving toward a purely epistemic goal will not advance pragmatic goals except by accident.

So formality, implementation are goals, where pragmaticity, epistemicity are kinds of goals?

Why do the scientists see these as promising? The reasons might have to do with computation or modelling, formalization which might have further applications in automatization, pragmatic reasons. I would describe some of the aforementioned as pragmatic reasons. The same goals can be epistemic as well. The goals of the social theorists are probably epistemic.

Constraining "the amount of further practical reasoning she must perform." \citep*{bratman_plans_1988}.


\section{Multi-agent systems and collective intentionality}
Misselhorn (2015) suggests applications of philosophical cooperation and collective agency for multi-agent systems in engineering, robotics, computer science and AI.

\subsection{Individualist vs collective frameworks in AI}

A theoretical argument for using collective rather than individualist frameworks in AI could be that we use abstract conceptual thinking as a way to lessen our cognitive workload. Levels of abstraction. The use of collective concepts is thus very important to us.




\subsection{Notes from Panzarasa}
Notes from \citeauthor{panzarasa_formalizing_2002}
A formal architecture built on top of \citet{wooldridge_intelligent_1995} with more nuanced social stuff.

p. 105 “High-level formal architecture for decision-making within a social context” with a hint of an implementation architecture.

p. 58 “Both individualistic and higher-order units of analysis and constructs are needed” (viitteet 5, 16, 17, 55, 63)): “individual agents both create and are products of social interaction”

p. 59 “to capture the dynamics of making a decision within a social setting we need representations that address higher-order units of analysis (i.e. groups, multi-agent actions, joint doxastic, motivational and deontic attitudes).

p. 79 The model is idealized (e.g. sequentiality).

p. 56 “we need to re-consider and extend our notions of practical reasoning and decision-making so that they deal with the inherently social aspects of these classes of multi-agent systems” ja p. 56 “CDM has generally been viewed and modelled as a kind of distributed reasoning and search, whereby a collection of agents collaboratively go through the search space of a problem in order to find a solution.” <- onko tämä siis “inherently social”? Ei jotenkin kauheen sosiaalinen kuvaus.

pp. 86 and 89 imply that intention is not commitment vs. \citealt{cohen_intention_1990} ”Intention is Choice with Commitment”


\subsection{Joint commitments and democracy theory}

\section{Constraining vs guiding choice (Bacharach) compered to top-down vs bottom-up solving in planning}
